{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothese 4: Influence of title and description\n",
    "\n",
    "1. We think that longer titles + descriptions perform better than shorter ones, because it is understood what the project is for\n",
    "2. There is an optimal length for titles + descriptions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up + Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries you need for your analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "# import sklearn items\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Import TfidfVectorizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,  CountVectorizer\n",
    "\n",
    "RSEED = 42\n",
    "\n",
    "# set general params\n",
    "plt.rcParams.update({ \"figure.figsize\" : (10, 5),\"axes.facecolor\" : \"white\", \"axes.edgecolor\":  \"black\"})\n",
    "plt.rcParams[\"figure.facecolor\"]= \"w\"\n",
    "pd.plotting.register_matplotlib_converters()\n",
    "# Floats (decimal numbers) should be displayed rounded with 1 decimal place\n",
    "pd.set_option('display.float_format', lambda x: '%.1f' % x)\n",
    "# Set style for plots\n",
    "plt.style.use('fivethirtyeight') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data \n",
    "df = pd.read_csv('data/2_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create frame with relevant items\n",
    "df_hypo4 = df[['id','name','name_length','description','description_length','state','category_main']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hypo4['description_length'] = df_hypo4['description_length'].fillna(0)\n",
    "df_hypo4['description'] = df_hypo4['description'].fillna('Misc')\n",
    "df_hypo4.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dummmies https://datagy.io/pandas-get-dummies/\n",
    "# Apply get_dummies to target of STATE\n",
    "target = [] # [] = list {} = dict\n",
    "\n",
    "dummies = pd.get_dummies(df['state'],columns=target, drop_first=True)\n",
    "dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply get dummies to added feature of CATEGORY\n",
    "category = [] # [] = list {} = dict\n",
    "\n",
    "category = pd.get_dummies(df['category_main'],columns=target, drop_first=True)\n",
    "category.head()\n",
    "\n",
    "frames = [df_hypo4,category,dummies]\n",
    "final_data = pd.concat(frames,axis=1)\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data[['name_length','description_length']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data[['successful','name_length','description_length']].query('successful == 1').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data[['successful','name_length','description_length']].query('successful == 0').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Median of name length:', final_data[['name_length']].median(),'Median of description length:', final_data[['description_length']].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mode of name length:', final_data[['name_length']].mode(),'Mode of description length:', final_data[['description_length']].mode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "his1 = sns.histplot(data=final_data,x='name_length',bins=10,hue='state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis1 = sns.displot(data=final_data,x='name_length',hue='state')\n",
    "print('Mean for description_length is:', round(df['name_length'].mean(),2))\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Median:' final_data['description_length'])\n",
    "his2 = sns.histplot(data=final_data,x='description_length',bins=10,hue='state')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis2 = sns.displot(data=final_data,x='description_length',hue='state')\n",
    "print('Mean for description_length is:', round(df['description_length'].mean(),2))\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(data=final_data[['name_length','description_length']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(final_data[['state','name_length','description_length']], hue=\"state\", height=3);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations = final_data[['name_length','description_length','successful']].corr()\n",
    "correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heat = sns.heatmap(correlations,annot=True)\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average length of description\n",
    "nlmean = final_data['name_length'].mean()\n",
    "print(nlmean)\n",
    "order = final_data[['category_main','name_length']].groupby('category_main').mean().sort_values('name_length', ascending=False)\n",
    "\n",
    "b = sns.barplot(data=final_data, x='category_main', y='name_length' , hue='state')\n",
    "b.set_xticklabels(b.get_xticklabels(),rotation = 90, size = 10)\n",
    "b.axhline(y=nlmean, color='black', linestyle =\"--\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylim(0, 150)\n",
    "plt.title(\"Average length of the project title by category\")\n",
    "plt.xlabel(\" \")\n",
    "plt.ylabel(\"Length of project's description\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average length of description\n",
    "dlmean = final_data['description_length'].mean()\n",
    "print()\n",
    "\n",
    "c = sns.barplot(data=final_data, x='category_main', y='description_length' , hue='state')\n",
    "c.set_xticklabels(b.get_xticklabels(),rotation = 90, size = 10)\n",
    "c.axhline(y=dlmean, color='black', linestyle =\"--\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.ylim(0, 150)\n",
    "plt.title(\"Average length of the project description by category\")\n",
    "plt.xlabel(\" \")\n",
    "plt.ylabel(\"Length of project's description\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis baseline model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Calculate logistic regression with only length features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining X and y\n",
    "y = final_data['successful']\n",
    "# Initial model logreg\n",
    "X = final_data[['name_length','description_length']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=RSEED, stratify=y)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling with MinMaxScaler\n",
    "\n",
    "# Try to scale you data with the MinMaxScaler() from sklearn. \n",
    "# It follows the same syntax as the StandardScaler.\n",
    "# Don't forget: you have to import the scaler at the top of your notebook. \n",
    "\n",
    "# Scaling with MinMaxScaler\n",
    "minmax = MinMaxScaler()\n",
    "X_train_scaled = minmax.fit_transform(X_train)\n",
    "X_test_scaled = minmax.transform(X_test)\n",
    "\n",
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is good practice to choose an evaluation method before running machine learning models - not after. The weighted average F1 score was chosen. The F1 score calculates the harmonic mean between precision and recall, and is a suitable measure because there is no preference for false positives or false negatives in this case (both are equally bad). The weighted average will be used because the classes are of slightly different sizes, and we want to be able to predict both successes and failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a logistic regression model with default parameters\n",
    "logreg = LogisticRegression()\n",
    "# Fit\n",
    "model1 = logreg.fit(X_train_scaled,y_train)\n",
    "# Predict\n",
    "y_pred = logreg.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 1 \n",
    "cr = classification_report(y_test, y_pred)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Confusion matrix using confusion_matrix from sklearn\n",
    "sns.heatmap(cm, cmap='YlGnBu', annot=True, fmt='d', linewidths=.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Lenth + Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow up model: \n",
    "X2 = final_data.drop(['successful','name','description','state','category_main'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into train and test sets\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y, test_size=0.3, random_state=RSEED, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling with MinMaxScaler\n",
    "minmax = MinMaxScaler()\n",
    "X_train_scaled2 = minmax.fit_transform(X_train2)\n",
    "X_test_scaled2 = minmax.transform(X_test2)\n",
    "\n",
    "X_train_scaled2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a logistic regression model with default parameters\n",
    "logreg = LogisticRegression()\n",
    "# Fit\n",
    "model2 = logreg.fit(X_train_scaled2,y_train2)\n",
    "# Predict\n",
    "y_pred2 = logreg.predict(X_test_scaled2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 2\n",
    "cr2 = classification_report(y_test2, y_pred2)\n",
    "\n",
    "cm2 = confusion_matrix(y_test2, y_pred2)\n",
    "\n",
    "print(cr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: Confusion matrix using confusion_matrix from sklearn\n",
    "sns.heatmap(cm2, cmap='YlGnBu', annot=True, fmt='d', linewidths=.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2: Naive bayes for title and description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and quality check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality chck for NaNs that might kill the model\n",
    "final_data[['successful','name','description']].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we have descriptions that are not filled, we cannot use these data, so either drop or fill and assign to new df (bayes)\n",
    "bayes = final_data[['successful','name','description']].dropna() # not working, so use fill as workaround\n",
    "#bayes = final_data[['successful','name','description']].fillna(\"Misc\")\n",
    "\n",
    "# Conduct quality check\n",
    "#bayes.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model1: Name of project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the features X for Bayes (Xb) and the target (yb)\n",
    "yb = bayes['successful']\n",
    "Xb = bayes['name']\n",
    "\n",
    "# Quality check for yb\n",
    "yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quality check for Xb\n",
    "Xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into train and test sets with 30%, since the sample is slightly unbalanced use stratify \n",
    "X_trainb, X_testb, y_trainb, y_testb = train_test_split(Xb, yb, test_size=0.3, random_state=RSEED, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pipeline with TfidfVectorizer and multinomial naive Bayes\n",
    "model_pipeline = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
    "\n",
    "# Fit pipeline/model with trainings data\n",
    "model_pipeline.fit(X_trainb, y_trainb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict data\n",
    "y_predb = model_pipeline.predict(X_testb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate \n",
    "crb = classification_report(y_testb, y_predb)\n",
    "\n",
    "cmb = confusion_matrix(y_testb, y_predb)\n",
    "\n",
    "print(crb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix using confusion_matrix from sklearn\n",
    "sns.heatmap(cmb, cmap='YlGnBu', annot=True, fmt='d', linewidths=.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model1: Description of project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the features X for Bayes (Xb) and the target (yb)\n",
    "ybd = bayes['successful']\n",
    "Xbd = bayes['description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting into train and test sets with 30%, since the sample is slightly unbalanced use stratify \n",
    "X_trainbd, X_testbd, y_trainbd, y_testbd = train_test_split(Xbd, ybd, test_size=0.3, random_state=RSEED, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit pipeline/model with trainings data\n",
    "model_pipeline.fit(X_trainbd, y_trainbd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict data\n",
    "y_predbd = model_pipeline.predict(X_testbd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate \n",
    "crbd = classification_report(y_testbd, y_predbd)\n",
    "\n",
    "cmbdd = confusion_matrix(y_testbd, y_predbd)\n",
    "\n",
    "print(crbd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix using confusion_matrix from sklearn\n",
    "sns.heatmap(cmbd, cmap='YlGnBu', annot=True, fmt='d', linewidths=.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial hypothesis was that the title and description hold predictive power to wether or not a project will be successful. \n",
    "\n",
    "In the initial check a logistic regression was used. The performance metrics of the model that included length of title and description where: \n",
    "- Accuracy: initial 0.58, after adding categories 0.64\n",
    "- F1 Score overall: 0.54, after adding categories 0.63\n",
    "\n",
    "The main driver for this result is the lack of the model to predict the unsuccessful projects correctly. \n",
    "- Considering only the sucessful predictions, we reach a satisfying F1 Score of 0.72. \n",
    "- However when looking at the unsuccessul projects we only get a F1 score of 0.52.\n",
    "\n",
    "Conclusion: Considering also the correlations, there seems to be a slight positive relationship between the length of the title and the outcome of the project. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the category does help improving precision, but it makes the recall worse. It can be concluded therefore that length needs additional factors to predict well. It was also tried to remove the length of the description, this did not change model greatly. Therefore it was decided to run a naive bayes to see if this holds true. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive Bayes (Multinomial)** was trained and it indicates that both the title and the description have a slight predictive power, when combined with the category of the project. \n",
    "\n",
    "Considerung only the title the accuracy can be improved to 0.65, mainly driven by the increase in the recall (0.85). The predictions for the negative values can slightly better, but still not good (precision of 0.66).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(crb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at only the description it is surprising to learn it also performs pretty good (Accuracy at 0.69) and slightly outperforms when it comes to the negative values (Precision at 0.71 for the true negatives). \n",
    "\n",
    "As a conclusion: both features should be included in the final model, since they hold predictive power over the successfulness of a project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(crbd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.8 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "acb36a9476365e98b039fbc63b261488d0140862f2956f6961bb0ecf49196712"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
